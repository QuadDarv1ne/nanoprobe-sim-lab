# -*- coding: utf-8 -*-
#!/usr/bin/env python3
#!/usr/bin/env python3
#!/usr/bin/env python3

"""
–ú–æ–¥—É–ª—å –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–Ω–æ–∑–æ–Ω–¥–∞
–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏
–ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
"""

import time
import threading
import json
import math
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Callable
from datetime import datetime, timedelta
import numpy as np
from scipy import stats
from dataclasses import dataclass
import pickle
import warnings

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.performance_profiler import PerformanceProfiler
from utils.resource_optimizer import ResourceManager
from utils.advanced_logger_analyzer import AdvancedLoggerAnalyzer
from utils.memory_tracker import MemoryTracker
from utils.performance_benchmark import PerformanceBenchmarkSuite
from utils.optimization_orchestrator import OptimizationOrchestrator
from utils.system_health_monitor import SystemHealthMonitor
from utils.performance_analytics_dashboard import PerformanceAnalyticsDashboard
from utils.performance_monitoring_center import PerformanceMonitoringCenter

@dataclass
class PredictionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    metric: str
    predicted_value: float
    confidence: float  # 0-1
    trend_direction: str  # 'increasing', 'decreasing', 'stable'
    time_horizon: timedelta
    recommendation: str

@dataclass
class AnomalyDetectionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π"""
    timestamp: datetime
    metric: str
    observed_value: float
    expected_value: float
    deviation: float
    severity: str  # 'low', 'medium', 'high', 'critical'
    is_anomaly: bool

class PredictiveAnalyticsEngine:
    """
    –ö–ª–∞—Å—Å –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
    """


    def __init__(self, output_dir: str = "predictive_analytics"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–≤–∏–∂–æ–∫ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

        Args:
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.performance_profiler = PerformanceProfiler(output_dir="profiles")
        self.resource_manager = ResourceManager()
        self.logger_analyzer = AdvancedLoggerAnalyzer()
        self.memory_tracker = MemoryTracker(output_dir="memory_logs")
        self.benchmark_suite = PerformanceBenchmarkSuite(output_dir="benchmarks")
        self.orchestrator = OptimizationOrchestrator(output_dir="optimization_reports")
        self.health_monitor = SystemHealthMonitor(output_dir="health_reports")
        self.analytics_dashboard = PerformanceAnalyticsDashboard(output_dir="analytics_reports")
        self.monitoring_center = PerformanceMonitoringCenter(output_dir="performance_monitoring")

        # –ò—Å—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.data_history = {}
        self.max_history_length = 10000  # –ú–∞–∫—Å–∏–º—É–º 10k —Ç–æ—á–µ–∫ –Ω–∞ –º–µ—Ç—Ä–∏–∫—É

        # –ú–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        self.models = {}
        self.model_training_needed = set()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        self.prediction_horizons = [5, 15, 30, 60]  # –º–∏–Ω—É—Ç—ã
        self.confidence_threshold = 0.8
        self.anomaly_threshold_multiplier = 2.0

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        self.active = False
        self.learning_thread = None
        self.prediction_thread = None

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'predictions_made': 0,
            'anomalies_detected': 0,
            'recommendations_applied': 0,
            'models_trained': 0
        }


    def add_data_point(self, metric_name: str, value: float, timestamp: Optional[datetime] = None):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ—á–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Args:
            metric_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            value: –ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            timestamp: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if timestamp is None:
            timestamp = datetime.now()

        if metric_name not in self.data_history:
            self.data_history[metric_name] = []

        self.data_history[metric_name].append((timestamp, value))

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.data_history[metric_name]) > self.max_history_length:
            self.data_history[metric_name] = self.data_history[metric_name][-self.max_history_length:]

        # –û—Ç–º–µ—á–∞–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏
        self.model_training_needed.add(metric_name)


    def get_recent_data(self, metric_name: str, minutes: int = 60) -> List[Tuple[datetime, float]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –Ω–µ–¥–∞–≤–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏

        Args:
            metric_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            minutes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∏–Ω—É—Ç –¥–ª—è –≤—ã–±–æ—Ä–∫–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –ø–∞—Ä (–≤—Ä–µ–º—è, –∑–Ω–∞—á–µ–Ω–∏–µ)
        """
        if metric_name not in self.data_history:
            return []

        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        recent_data = [(t, v) for t, v in self.data_history[metric_name] if t >= cutoff_time]

        return sorted(recent_data, key=lambda x: x[0])  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏


    def train_linear_model(self, metric_name: str) -> Optional[Dict[str, float]]:
        """
        –û–±—É—á–∞–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏

        Args:
            metric_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏

        Returns:
            –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (slope, intercept, r_squared) –∏–ª–∏ None
        """
        data = self.get_recent_data(metric_name, minutes=120)  # 2 —á–∞—Å–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        if len(data) < 10:  # –ú–∏–Ω–∏–º—É–º 10 —Ç–æ—á–µ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            return None

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Ä–µ–º—è –≤ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã —Å –Ω–∞—á–∞–ª–∞)
        start_time = data[0][0]
        x_values = [(t - start_time).total_seconds() for t, _ in data]
        y_values = [v for _, v in data]

        # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
        slope, intercept, r_value, p_value, std_err = stats.linregress(x_values, y_values)

        return {
            'slope': slope,
            'intercept': intercept,
            'r_squared': r_value ** 2,
            'std_error': std_err,
            'sample_size': len(data)
        }


    def predict_value(self, model_params: Dict[str, float], minutes_ahead: int) -> Tuple[float, float]:
        """
        –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏

        Args:
            model_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            minutes_ahead: –ù–∞ —Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –≤–ø–µ—Ä–µ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å

        Returns:
            (–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª)
        """
        seconds_ahead = minutes_ahead * 60
        predicted_value = model_params['intercept'] + model_params['slope'] * seconds_ahead

        # –û—Ü–µ–Ω–∫–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
        confidence_interval = model_params['std_error'] * 1.96  # 95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª

        return predicted_value, confidence_interval


    def detect_anomalies(self, metric_name: str, window_minutes: int = 30) -> List[AnomalyDetectionResult]:
        """
        –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –º–µ—Ç—Ä–∏–∫–µ

        Args:
            metric_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            window_minutes: –û–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
        """
        recent_data = self.get_recent_data(metric_name, minutes=window_minutes)
        if len(recent_data) < 10:
            return []

        values = [v for _, v in recent_data]
        mean_val = np.mean(values)
        std_val = np.std(values)

        anomalies = []
        for timestamp, observed_value in recent_data[-5:]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π
            expected_value = mean_val
            deviation = abs(observed_value - expected_value)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            if deviation > self.anomaly_threshold_multiplier * 2 * std_val:
                severity = 'critical'
            elif deviation > self.anomaly_threshold_multiplier * 1.5 * std_val:
                severity = 'high'
            elif deviation > self.anomaly_threshold_multiplier * std_val:
                severity = 'medium'
            elif deviation > self.anomaly_threshold_multiplier * 0.5 * std_val:
                severity = 'low'
            else:
                severity = 'normal'

            is_anomaly = severity != 'normal'

            if is_anomaly:
                anomalies.append(AnomalyDetectionResult(
                    timestamp=timestamp,
                    metric=metric_name,
                    observed_value=observed_value,
                    expected_value=expected_value,
                    deviation=deviation,
                    severity=severity,
                    is_anomaly=is_anomaly
                ))

        return anomalies


    def generate_prediction(self, metric_name: str, minutes_ahead: int) -> Optional[PredictionResult]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏

        Args:
            metric_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            minutes_ahead: –ù–∞ —Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –≤–ø–µ—Ä–µ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ –∏–ª–∏ None
        """
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if metric_name in self.model_training_needed:
            model_params = self.train_linear_model(metric_name)
            if model_params:
                self.models[metric_name] = model_params
                self.model_training_needed.discard(metric_name)
                self.stats['models_trained'] += 1
            else:
                # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –æ–±—É—á–∏—Ç—å –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
                recent_data = self.get_recent_data(metric_name, minutes=30)
                if recent_data:
                    avg_value = np.mean([v for _, v in recent_data])
                    model_params = {
                        'slope': 0,
                        'intercept': avg_value,
                        'r_squared': 0.1,  # –ù–∏–∑–∫–∏–π R¬≤ –¥–ª—è —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
                        'std_error': np.std([v for _, v in recent_data]),
                        'sample_size': len(recent_data)
                    }
                    self.models[metric_name] = model_params

        if metric_name not in self.models:
            return None

        model_params = self.models[metric_name]
        predicted_value, confidence_interval = self.predict_value(model_params, minutes_ahead)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
        trend_direction = 'stable'
        if model_params['slope'] > 0.01:  # –ü–æ—Ä–æ–≥ –¥–ª—è –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞
            trend_direction = 'increasing'
        elif model_params['slope'] < -0.01:  # –ü–æ—Ä–æ–≥ –¥–ª—è —É–±—ã–≤–∞—é—â–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞
            trend_direction = 'decreasing'

        # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ–≤–µ—Ä–∏–µ –∫ –ø—Ä–æ–≥–Ω–æ–∑—É
        confidence = min(1.0, model_params['r_squared'] + 0.1)  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –±–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
        recommendation = self._generate_recommendation(metric_name, predicted_value, trend_direction)

        result = PredictionResult(
            metric=metric_name,
            predicted_value=predicted_value,
            confidence=confidence,
            trend_direction=trend_direction,
            time_horizon=timedelta(minutes=minutes_ahead),
            recommendation=recommendation
        )

        self.stats['predictions_made'] += 1

        return result


    def _generate_recommendation(self, metric_name: str, predicted_value: float, trend_direction: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥–Ω–æ–∑–∞

        Args:
            metric_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            predicted_value: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            trend_direction: –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞

        Returns:
            –¢–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        """
        if 'cpu' in metric_name.lower():
            if predicted_value > 80 and trend_direction == 'increasing':
                return "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è –≤—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Ä–µ—Å—É—Ä—Å–æ–≤"
            elif predicted_value > 70 and trend_direction == 'increasing':
                return "–í–æ–∑–º–æ–∂–µ–Ω —Ä–æ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ CPU, –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ—Å—å –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"
            elif predicted_value < 20 and trend_direction == 'decreasing':
                return "–ù–∏–∑–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU, –≤–æ–∑–º–æ–∂–Ω–æ, –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"

        elif 'memory' in metric_name.lower() or 'ram' in metric_name.lower():
            if predicted_value > 85 and trend_direction == 'increasing':
                return "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–∞–º—è—Ç–∏"
            elif predicted_value > 75 and trend_direction == 'increasing':
                return "–í–æ–∑–º–æ–∂–µ–Ω —Ä–æ—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏, –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ—Å—å –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"

        elif 'efficiency' in metric_name.lower() or 'score' in metric_name.lower():
            if predicted_value < 70 and trend_direction == 'decreasing':
                return "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è —Å–Ω–∏–∂–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é"
            elif predicted_value < 80 and trend_direction == 'decreasing':
                return "–í–æ–∑–º–æ–∂–µ–Ω —Å–ø–∞–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ—Å—å –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"

        return "–ü—Ä–æ–≥–Ω–æ–∑ –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è"


    def get_predictive_insights(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–º–∏ –∏–Ω—Å–∞–π—Ç–∞–º–∏
        """
        insights = {
            'predictions': {},
            'anomalies': {},
            'trends': {},
            'recommendations': [],
            'confidence_levels': {},
            'timestamp': datetime.now().isoformat()
        }

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        metrics_to_predict = [
            'cpu_percent', 'memory_percent', 'resource_efficiency',
            'optimization_score', 'active_processes'
        ]

        for metric in metrics_to_predict:
            predictions = {}
            for horizon in [5, 15, 30]:  # –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ 5, 15 –∏ 30 –º–∏–Ω—É—Ç
                pred_result = self.generate_prediction(metric, horizon)
                if pred_result and pred_result.confidence > 0.5:
                    predictions[f'{horizon}_min'] = {
                        'predicted_value': pred_result.predicted_value,
                        'confidence': pred_result.confidence,
                        'trend': pred_result.trend_direction,
                        'recommendation': pred_result.recommendation
                    }

            if predictions:
                insights['predictions'][metric] = predictions
                insights['confidence_levels'][metric] = np.mean([p['confidence'] for p in predictions.values()])

        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        for metric in metrics_to_predict[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            anomalies = self.detect_anomalies(metric)
            if anomalies:
                insights['anomalies'][metric] = [
                    {
                        'timestamp': a.timestamp.isoformat(),
                        'severity': a.severity,
                        'observed': a.observed_value,
                        'expected': a.expected_value,
                        'deviation': a.deviation
                    }
                    for a in anomalies
                ]
                insights['anomalies'][metric] = insights['anomalies'][metric][-5:]  # –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5

        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        for metric in metrics_to_predict:
            if metric in self.models:
                model = self.models[metric]
                insights['trends'][metric] = {
                    'slope': model['slope'],
                    'r_squared': model['r_squared'],
                    'trend_direction': 'increasing' if model['slope'] > 0.01 else 'decreasing' if model['slope'] < -0.01 else 'stable'
                }

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        high_priority_recs = []
        for metric, preds in insights['predictions'].items():
            for timeframe, pred_data in preds.items():
                if pred_data['confidence'] > 0.7:  # –í—ã—Å–æ–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å
                    high_priority_recs.append(pred_data['recommendation'])

        insights['recommendations'] = high_priority_recs

        return insights


    def auto_apply_recommendations(self) -> Dict[str, Any]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        insights = self.get_predictive_insights()
        applied_actions = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ –≤—ã—Å–æ–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É CPU
        if 'cpu_percent' in insights['predictions']:
            cpu_preds = insights['predictions']['cpu_percent']
            for timeframe, pred_data in cpu_preds.items():
                if pred_data['predicted_value'] > 85 and pred_data['trend'] == 'increasing':
                    print(f"‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è –≤—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU ({pred_data['predicted_value']:.1f}%), –∑–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
                    result = self.resource_manager.optimize_cpu_usage()
                    applied_actions.append({
                        'action': 'cpu_optimization',
                        'trigger': f"Predicted high CPU load: {pred_data['predicted_value']:.1f}%",
                        'result': result
                    })
                    self.stats['recommendations_applied'] += 1
                    break  # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫—É

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        if 'memory_percent' in insights['predictions']:
            mem_preds = insights['predictions']['memory_percent']
            for timeframe, pred_data in mem_preds.items():
                if pred_data['predicted_value'] > 90 and pred_data['trend'] == 'increasing':
                    print(f"‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ ({pred_data['predicted_value']:.1f}%), –∑–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
                    result = self.memory_tracker.perform_memory_optimization()
                    applied_actions.append({
                        'action': 'memory_optimization',
                        'trigger': f"Predicted high memory usage: {pred_data['predicted_value']:.1f}%",
                        'result': result
                    })
                    self.stats['recommendations_applied'] += 1
                    break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ —Å–Ω–∏–∂–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        if 'resource_efficiency' in insights['predictions']:
            eff_preds = insights['predictions']['resource_efficiency']
            for timeframe, pred_data in eff_preds.items():
                if pred_data['predicted_value'] < 70 and pred_data['trend'] == 'decreasing':
                    print(f"‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è —Å–Ω–∏–∂–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ ({pred_data['predicted_value']:.1f}%), –∑–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
                    result = self.orchestrator.start_comprehensive_optimization(["core_utils"])
                    applied_actions.append({
                        'action': 'comprehensive_optimization',
                        'trigger': f"Predicted low efficiency: {pred_data['predicted_value']:.1f}%",
                        'result': result
                    })
                    self.stats['recommendations_applied'] += 1
                    break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
        for metric, anomalies in insights.get('anomalies', {}).items():
            for anomaly in anomalies[-2:]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –∞–Ω–æ–º–∞–ª–∏–∏
                if anomaly['severity'] in ['high', 'critical']:
                    print(f"üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–æ–º–∞–ª–∏—è –≤ {metric}, –∑–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏...")
                    applied_actions.append({
                        'action': 'diagnostic_scan',
                        'trigger': f"Critical anomaly in {metric}: {anomaly['severity']}",
                        'result': f"Anomaly detected at {anomaly['timestamp']}"
                    })
                    self.stats['recommendations_applied'] += 1

        return {
            'applied_actions': applied_actions,
            'total_actions': len(applied_actions),
            'timestamp': datetime.now().isoformat()
        }


    def start_predictive_monitoring(self, collection_interval: float = 10.0, prediction_interval: float = 60.0):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

        Args:
            collection_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (—Å–µ–∫—É–Ω–¥—ã)
            prediction_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)
        """
        if self.active:
            return

        self.active = True

        def data_collection_loop():
    """TODO: Add description"""

            while self.active:
                try:
                    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                    metrics = self.monitoring_center.get_current_metrics()

                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
                    for metric_name, value in metrics.items():
                        if isinstance(value, (int, float)):
                            self.add_data_point(metric_name, value)

                    # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
                    for metric_name, value in metrics.items():
                        if isinstance(value, (int, float)):
                            anomalies = self.detect_anomalies(metric_name, window_minutes=10)
                            if anomalies:
                                self.stats['anomalies_detected'] += len(anomalies)

                    time.sleep(collection_interval)
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
                    time.sleep(collection_interval)

    """TODO: Add description"""

        def prediction_loop():
            """TODO: Add description"""
            while self.active:
                try:
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
                    recommendations = self.auto_apply_recommendations()

                    time.sleep(prediction_interval)
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                    time.sleep(prediction_interval)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏
        self.learning_thread = threading.Thread(target=data_collection_loop, daemon=True)
        self.prediction_thread = threading.Thread(target=prediction_loop, daemon=True)

        self.learning_thread.start()
        self.prediction_thread.start()

        print("üß† –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω")


    def stop_predictive_monitoring(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.active = False
        if self.learning_thread:
            self.learning_thread.join(timeout=2.0)
        if self.prediction_thread:
            self.prediction_thread.join(timeout=2.0)

        print("üõë –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


    def save_models(self, filepath: Optional[str] = None):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

        Args:
            filepath: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if filepath is None:
            filepath = str(self.output_dir / f"predictive_models_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl")

        models_data = {
            'models': self.models,
            'data_history': self.data_history,
            'stats': self.stats,
            'timestamp': datetime.now().isoformat()
        }

        with open(filepath, 'wb') as f:
            pickle.dump(models_data, f)

        print(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")


    def load_models(self, filepath: str):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–µ–π
        """
        with open(filepath, 'rb') as f:
            models_data = pickle.load(f)

        self.models = models_data.get('models', {})
        self.data_history = models_data.get('data_history', {})
        self.stats.update(models_data.get('stats', {}))

        print(f"üìÇ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {filepath}")


    def get_performance_summary(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–µ

        Returns:
            –°–≤–æ–¥–∫–∞ –ø–æ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–µ
        """
        insights = self.get_predictive_insights()

        summary = {
            'stats': self.stats,
            'insights': insights,
            'model_count': len(self.models),
            'metrics_tracked': list(self.data_history.keys()),
            'prediction_accuracy': {
                metric: model['r_squared'] for metric, model in self.models.items()
            },
            'timestamp': datetime.now().isoformat()
        }

        return summary

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
    print("=== –ü–†–ï–î–ò–ö–¢–ò–í–ù–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò ===")
    print("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏...")

    # –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
    engine = PredictiveAnalyticsEngine(output_dir="predictive_analytics")

    print("‚úÖ –î–≤–∏–∂–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è
    print("üìä –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    base_time = datetime.now()
    for i in range(20):
        offset_time = base_time - timedelta(minutes=i*5)
        engine.add_data_point('cpu_percent', 30 + np.random.normal(0, 5), offset_time)
        engine.add_data_point('memory_percent', 45 + np.random.normal(0, 3), offset_time)
        engine.add_data_point('resource_efficiency', 85 + np.random.normal(0, 2), offset_time)

    print("‚úÖ –ù–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã")

    # –ü—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑
    print("\nüîÆ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–≤—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤...")
    cpu_pred = engine.generate_prediction('cpu_percent', 10)
    if cpu_pred:
        print(f"   –ü—Ä–æ–≥–Ω–æ–∑ CPU —á–µ—Ä–µ–∑ 10 –º–∏–Ω: {cpu_pred.predicted_value:.2f}% (–¥–æ–≤–µ—Ä–∏–µ: {cpu_pred.confidence:.2f})")

    mem_pred = engine.generate_prediction('memory_percent', 10)
    if mem_pred:
        print(f"   –ü—Ä–æ–≥–Ω–æ–∑ –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ 10 –º–∏–Ω: {mem_pred.predicted_value:.2f}% (–¥–æ–≤–µ—Ä–∏–µ: {mem_pred.confidence:.2f})")

    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Å–∞–π—Ç—ã
    print("\nüí° –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤...")
    insights = engine.get_predictive_insights()

    print(f"   –ü—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è –º–µ—Ç—Ä–∏–∫: {list(insights['predictions'].keys())}")
    print(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏: {list(insights['anomalies'].keys())}")
    print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {len(insights['recommendations'])}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑–æ–≤ —Å–¥–µ–ª–∞–Ω–æ: {engine.stats['predictions_made']}")
    print(f"   ‚Ä¢ –ê–Ω–æ–º–∞–ª–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {engine.stats['anomalies_detected']}")
    print(f"   ‚Ä¢ –û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {engine.stats['models_trained']}")

    print(f"\nüîó –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:")
    print("   ‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ: engine.generate_prediction()")
    print("   ‚Ä¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π: engine.detect_anomalies()")
    print("   ‚Ä¢ –ò–Ω—Å–∞–π—Ç—ã: engine.get_predictive_insights()")
    print("   ‚Ä¢ –ê–≤—Ç–æ-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: engine.auto_apply_recommendations()")
    print("   ‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: engine.start_predictive_monitoring()")

    print("\nüéâ –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")

if __name__ == "__main__":
    main()

